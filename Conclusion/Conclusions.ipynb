{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What we learn at the Multimodal Learning Analytics Workshop\n",
    "=============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan Multimodal Learning Analytics Studies ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MDC.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture and Process Different Modalities #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Images ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "import requests\n",
    "import cv2\n",
    "import operator\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import clear_output\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "import xml.etree.ElementTree as etree \n",
    "from pylab import *\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "from IPython.display import Image  \n",
    "from sklearn.externals.six import StringIO \n",
    "import pydotplus\n",
    "\n",
    "# Define the API to use\n",
    "_url = 'https://api.projectoxford.ai/emotion/v1.0/recognize'\n",
    "_key = 'c0958b1e63134377960c2796e82aa14e'\n",
    "_maxNumRetries = 10\n",
    "# Functions to use the API\n",
    "def processRequest( json, data, headers ):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to process the request to Project Oxford\n",
    "\n",
    "    Parameters:\n",
    "    json: Used when processing images from its URL. See API Documentation\n",
    "    data: Used when processing image read from disk. See API Documentation\n",
    "    headers: Used to pass the key information and the data type request\n",
    "    \"\"\"\n",
    "\n",
    "    retries = 0\n",
    "    result = None\n",
    "\n",
    "    while True:\n",
    "\n",
    "        response = requests.request( 'post', _url, json = json, data = data, headers = headers, params = None )\n",
    "\n",
    "        if response.status_code == 429: \n",
    "\n",
    "            print(\"Message: %s\" % ( response.json()['error']['message'] ))\n",
    "\n",
    "            if retries <= _maxNumRetries: \n",
    "                time.sleep(1) \n",
    "                retries += 1\n",
    "                continue\n",
    "            else: \n",
    "                print('Error: failed after retrying!')\n",
    "                break\n",
    "\n",
    "        elif response.status_code == 200 or response.status_code == 201:\n",
    "\n",
    "            if 'content-length' in response.headers and int(response.headers['content-length']) == 0: \n",
    "                result = None \n",
    "            elif 'content-type' in response.headers and isinstance(response.headers['content-type'], str): \n",
    "                if 'application/json' in response.headers['content-type'].lower(): \n",
    "                    result = response.json() if response.content else None \n",
    "                elif 'image' in response.headers['content-type'].lower(): \n",
    "                    result = response.content\n",
    "        else:\n",
    "            print(\"Error code: %d\" % ( response.status_code ))\n",
    "            print(\"Message: %s\" % ( response.json()['error']['message'] ))\n",
    "\n",
    "        break\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Fuction to add the emotion to the image\n",
    "def renderResultOnImage( result, img ):\n",
    "    \n",
    "    \"\"\"Display the obtained results onto the input image\"\"\"\n",
    "    \n",
    "    for currFace in result:\n",
    "        faceRectangle = currFace['faceRectangle']\n",
    "        cv2.rectangle( img,(faceRectangle['left'],faceRectangle['top']),\n",
    "                           (faceRectangle['left']+faceRectangle['width'], faceRectangle['top'] + faceRectangle['height']),\n",
    "                       color = (255,0,0), thickness = 5 )\n",
    "\n",
    "\n",
    "    for currFace in result:\n",
    "        faceRectangle = currFace['faceRectangle']\n",
    "        currEmotion = max(currFace['scores'].items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "\n",
    "        textToWrite = \"%s\" % ( currEmotion )\n",
    "        cv2.putText( img, textToWrite, (faceRectangle['left'],faceRectangle['top']-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read a static image and assign emotion\n",
    "pathToFileInDisk = r'MLA.png'\n",
    "with open( pathToFileInDisk, 'rb' ) as f:\n",
    "    data = f.read()\n",
    "\n",
    "headers = dict()\n",
    "headers['Ocp-Apim-Subscription-Key'] = _key\n",
    "headers['Content-Type'] = 'application/octet-stream'\n",
    "\n",
    "json = None\n",
    "\n",
    "result = processRequest( json, data, headers )\n",
    "\n",
    "for currFace in result:\n",
    "        faceRectangle = currFace['faceRectangle']\n",
    "        currEmotion = max(currFace['scores'].items(), key=operator.itemgetter(1))[0]\n",
    "        print(currEmotion)\n",
    "\n",
    "\n",
    "\n",
    "# Load the original image from disk\n",
    "data8uint = np.fromstring( data, np.uint8 ) # Convert string to an unsigned int array\n",
    "img = cv2.cvtColor( cv2.imdecode( data8uint, cv2.IMREAD_COLOR ), cv2.COLOR_BGR2RGB )\n",
    "\n",
    "renderResultOnImage( result, img )\n",
    "\n",
    "ig, ax = plt.subplots(figsize=(15, 20))\n",
    "ax.imshow( img )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Video ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We obtain the object to be tracked (calculator)\n",
    "MIN_MATCH_COUNT = 2\n",
    "\n",
    "cap = cv2.VideoCapture('MLA2.mp4')\n",
    "img1 = cv2.imread(\"MLA2.png\",0)  # queryImage\n",
    "img1show = cv2.cvtColor(img1, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "frame = 0\n",
    "\n",
    "#For each frame\n",
    "while(frame<200):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES,frame)\n",
    "    ret, img2 = cap.read()\n",
    "    gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    x = -1\n",
    "    y = -1\n",
    "    degreeCorrection = 20\n",
    "    student = -1\n",
    "\n",
    "    frame = frame + 25\n",
    "\n",
    "    #Create a SIFT feature extractor\n",
    "    try:\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "        kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "        FLANN_INDEX_KDTREE = 0\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks = 50)\n",
    "\n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "        matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "        good = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < 0.7*n.distance:\n",
    "                good.append(m)\n",
    "\n",
    "\n",
    "        if len(good)>=MIN_MATCH_COUNT:\n",
    "            src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "            dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "            TM=cv2.estimateRigidTransform(src_pts, dst_pts,False)\n",
    "            newrow = [0,0,1]\n",
    "            TM = np.vstack([TM, newrow])\n",
    "\n",
    "            h,w = img1.shape\n",
    "            pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "            dst = cv2.perspectiveTransform(pts,TM)\n",
    "\n",
    "            cv2.polylines(img2,[np.int32(dst)],True,(255,255,255),5)\n",
    "            degree = math.degrees(math.acos(TM[0][0]))\n",
    "            degree=degree+degreeCorrection\n",
    "\n",
    "            centerx = (dst[0][0][0]+dst[2][0][0])/2\n",
    "            centery = (dst[0][0][1]+dst[2][0][1])/2\n",
    "\n",
    "            x = centerx\n",
    "            y = centery\n",
    "            cv2.circle(img2, (np.int32(centerx),np.int32(centery)), 10, (255,255,255),10) \n",
    "\n",
    "            #student = 0\n",
    "            height,width = img2.shape[0],img2.shape[1]\n",
    "            n = (centery) - (math.tan(math.pi/2-math.acos(TM[0][0])) * (width-centerx))\n",
    "            x0=(-n)/math.tan(math.radians(90-degree))\n",
    "\n",
    "            xf = x\n",
    "            yf = y/2\n",
    "\n",
    "            if x0>-1 and x0<width+1:\n",
    "                if x0<180:\n",
    "                    student = 1\n",
    "                if x0>179 and x0<701:\n",
    "                    student = 2\n",
    "                if x0>700:\n",
    "                    student = 3\n",
    "            else:\n",
    "                if x0<0:\n",
    "                    student = 1\n",
    "                if x0>width:\n",
    "                    student = 3\n",
    "\n",
    "            cv2.line(img2, (np.int32(centerx),np.int32(centery)), (np.int32(width-x0),0), (237,19,19),5)\n",
    "\n",
    "        else:\n",
    "            print(\"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT))\n",
    "    except:\n",
    "        pass \n",
    "\n",
    "   \n",
    "    fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    img2show = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img2show)\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Audio ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from os import listdir\n",
    "directory=\"afiles\"\n",
    "\n",
    "text=\"\"\n",
    "r = sr.Recognizer()\n",
    "files=listdir(directory)\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    if not \"audio\" in file:\n",
    "         files.remove(file)\n",
    "for i in range(1,len(files)+1):\n",
    "    print(i)\n",
    "    AUDIO_FILE = directory+\"/\"+\"audiofile\"+str(i)+\".wav\"\n",
    "    with sr.AudioFile(AUDIO_FILE) as source:\n",
    "        audio = r.record(source) # read the entire audio file\n",
    "        # use the audio file as the audio source\n",
    "    # recognize speech using Microsoft Bing Voice Recognition\n",
    "    BING_KEY = \"8638a1837c0946f799ecd77e011d97d9\" # Microsoft Bing Voice Recognition API keys 32-character lowercase hexadecimal strings\n",
    "    try:\n",
    "        text = r.recognize_bing(audio, key=BING_KEY)\n",
    "        print(\"Audio recognized\")\n",
    "        print(\"Recognized text:\", text)\n",
    "        \n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Audio not understood\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Error with service; {0}\".format(e))\n",
    "    time.sleep(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Pens #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory=\"digipen/\"\n",
    "student=\"S1\"\n",
    "treeSessions = etree.parse(directory+student+\"/\"+\"data_session.xml\")\n",
    "rootSessions=treeSessions.getroot()\n",
    "notes = rootSessions[0]\n",
    "traces =[]\n",
    "pages = []\n",
    "for note in notes:\n",
    "    page=[]\n",
    "    startime = note.get('starttime')\n",
    "    endtime = note.get('endtime')\n",
    "    notefile = note[0].text\n",
    "    treeNote = etree.parse(directory+student+\"/\"+notefile)\n",
    "    rootNote = treeNote.getroot()\n",
    "    noteinfo = rootNote[0]\n",
    "    for trace in noteinfo:\n",
    "        tracedata=[]\n",
    "        for point in trace:\n",
    "            pointdata={}\n",
    "            pointdata[\"x\"] = float(point[0].text)\n",
    "            pointdata[\"y\"] = float(point[1].text)\n",
    "            pointdata[\"timestamp\"] = long(point[2].text)\n",
    "            pointdata[\"force\"] = float(point[3].text)\n",
    "            tracedata.append(pointdata)\n",
    "        traces.append(tracedata)\n",
    "        page.append(tracedata)\n",
    "    pages.append(page)\n",
    "\n",
    "for page in pages:\n",
    "    figure(figsize=(10*1.5,7*1.5))\n",
    "    for trace in page:\n",
    "        x =[]\n",
    "        y=[]\n",
    "        for point in trace:\n",
    "            x.append(int(point.get(\"x\")))\n",
    "            y.append(int(point.get(\"y\")))\n",
    "        plot(y,x)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion and Decisions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"DataExpert.csv\",sep=\",\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=['PCU', 'DHT', 'SDDHT', 'TM', 'NOI', 'TSD', 'TNM', 'PNM', 'TMTM', 'PMTM', 'TCP', 'TNS', 'NOL', 'NOR', \n",
    "               'NOC', 'NOE', 'NOA', 'ASPL', 'ANP', 'ASD', 'ASTL', 'ASP', 'SS', 'MN', 'MM', 'MC', 'MMO', \n",
    "               'LMO', 'LMV', 'MTM', 'MI']\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(data[features], data['Expert'])\n",
    "dot_data = StringIO()  \n",
    "tree.export_graphviz(clf, out_file=dot_data, feature_names=features)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
